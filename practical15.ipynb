{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mYXYytH-4VDn"
      },
      "outputs": [],
      "source": [
        "# Step 1: Import nltk and heapq\n",
        "import nltk\n",
        "import heapq\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Download necessary NLTK data\n",
        "nltk.download('punkt')        # For tokenization\n",
        "nltk.download('stopwords')     # For stopwords list\n",
        "nltk.download('punkt_tab')     # Additional punkt data\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "leg_eUiJ4V1-",
        "outputId": "71a6c8ff-6817-41ee-c0c4-62fc81da934e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Load or input text document\n",
        "def load_text():\n",
        "    print(\"Choose input method:\")\n",
        "    print(\"1. Enter text manually\")\n",
        "    print(\"2. Read from file\")\n",
        "\n",
        "    choice = input(\"Enter your choice (1 or 2): \")\n",
        "\n",
        "    if choice == '1':\n",
        "        print(\"\\nEnter your text (press Enter twice to finish):\")\n",
        "        lines = []\n",
        "        while True:\n",
        "            line = input()\n",
        "            if line:\n",
        "                lines.append(line)\n",
        "            else:\n",
        "                break\n",
        "        text = ' '.join(lines)\n",
        "        return text\n",
        "\n",
        "    elif choice == '2':\n",
        "        filename = input(\"Enter filename (e.g., sample.txt): \")\n",
        "        try:\n",
        "            with open(filename, 'r', encoding='utf-8') as file:\n",
        "                text = file.read()\n",
        "            return text\n",
        "        except FileNotFoundError:\n",
        "            print(\"File not found!\")\n",
        "            return None\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid choice!\")\n",
        "        return None\n",
        "\n",
        "# Sample text for testing\n",
        "sample_text = \"\"\"\n",
        "Artificial Intelligence (AI) is transforming the world at an unprecedented pace.\n",
        "From healthcare to transportation, AI technologies are being integrated into various\n",
        "sectors to improve efficiency and accuracy. Machine learning, a subset of AI, enables\n",
        "systems to learn and improve from experience without being explicitly programmed.\n",
        "Deep learning, a more advanced form of machine learning, uses neural networks with\n",
        "multiple layers to analyze various factors of data. These technologies have led to\n",
        "breakthroughs in image recognition, natural language processing, and autonomous vehicles.\n",
        "However, the rapid advancement of AI also raises ethical concerns about privacy,\n",
        "job displacement, and decision-making transparency. Many experts argue for the need\n",
        "to develop responsible AI frameworks that ensure these technologies benefit humanity\n",
        "while minimizing potential harm. As AI continues to evolve, it will be crucial to\n",
        "strike a balance between innovation and regulation. The future of AI holds immense\n",
        "potential, but it must be guided by ethical principles and human values.\n",
        "\"\"\"\n",
        "\n",
        "# Load text\n",
        "text = load_text()\n",
        "if text is None:\n",
        "    print(\"Using sample text instead...\")\n",
        "    text = sample_text\n",
        "\n",
        "print(\"\\nOriginal Text:\")\n",
        "print(\"-\" * 50)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73VycT764WWu",
        "outputId": "5d4e014f-c82c-4827-9329-d1f8de399e5c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Choose input method:\n",
            "1. Enter text manually\n",
            "2. Read from file\n",
            "Enter your choice (1 or 2): 1\n",
            "\n",
            "Enter your text (press Enter twice to finish):\n",
            "\n",
            "\n",
            "Original Text:\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Tokenize text into sentences and words\n",
        "\n",
        "# Tokenize into sentences\n",
        "sentences = sent_tokenize(text)\n",
        "print(f\"\\nNumber of sentences: {len(sentences)}\")\n",
        "print(\"\\nSentences:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sentence[:50]}...\")\n",
        "\n",
        "# Tokenize into words\n",
        "words = word_tokenize(text.lower())\n",
        "print(f\"\\nNumber of words: {len(words)}\")\n",
        "print(f\"First 20 words: {words[:20]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-x6f8ru4WwV",
        "outputId": "57b7c6ed-41bf-4897-d9b6-db3d334f8261"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of sentences: 0\n",
            "\n",
            "Sentences:\n",
            "\n",
            "Number of words: 0\n",
            "First 20 words: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Compute word frequencies, ignoring stopwords\n",
        "\n",
        "# Get stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(f\"\\nStopwords sample: {list(stop_words)[:10]}\")\n",
        "\n",
        "# Calculate word frequencies\n",
        "word_frequencies = {}\n",
        "\n",
        "for word in words:\n",
        "    if word not in stop_words and word.isalnum():  # Ignore stopwords and punctuation\n",
        "        if word not in word_frequencies:\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1\n",
        "\n",
        "print(f\"\\nUnique words after removing stopwords: {len(word_frequencies)}\")\n",
        "\n",
        "# Normalize frequencies (divide by maximum frequency)\n",
        "if word_frequencies: # Add this check\n",
        "    max_frequency = max(word_frequencies.values())\n",
        "    for word in word_frequencies:\n",
        "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
        "else:\n",
        "    print(\"\\nCannot normalize word frequencies: No words were processed (text might be empty or contain only stopwords/punctuation).\")\n",
        "    # Handle the empty case appropriately if subsequent steps rely on a normalized frequency.\n",
        "\n",
        "# Show top 10 words by frequency\n",
        "print(\"\\nTop 10 important words:\")\n",
        "if word_frequencies: # Add this check for printing top words\n",
        "    sorted_words = sorted(word_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "    for word, freq in sorted_words[:10]:\n",
        "        print(f\"{word}: {freq:.3f}\")\n",
        "else:\n",
        "    print(\"No words to display.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2clyb9-I4XN6",
        "outputId": "b55ba492-6f61-4ab9-da02-7fb31fa995f5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stopwords sample: ['until', 'been', \"doesn't\", 'herself', \"i'll\", \"hasn't\", 'hasn', \"needn't\", 'above', 'm']\n",
            "\n",
            "Unique words after removing stopwords: 0\n",
            "\n",
            "Cannot normalize word frequencies: No words were processed (text might be empty or contain only stopwords/punctuation).\n",
            "\n",
            "Top 10 important words:\n",
            "No words to display.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Score each sentence based on important words\n",
        "\n",
        "sentence_scores = {}\n",
        "\n",
        "for sentence in sentences:\n",
        "    # Check sentence length (avoid very long sentences)\n",
        "    if len(sentence.split()) < 30:\n",
        "        for word in word_tokenize(sentence.lower()):\n",
        "            if word in word_frequencies:\n",
        "                if sentence not in sentence_scores:\n",
        "                    sentence_scores[sentence] = word_frequencies[word]\n",
        "                else:\n",
        "                    sentence_scores[sentence] += word_frequencies[word]\n",
        "\n",
        "print(\"\\nSentence Scores:\")\n",
        "# Show scores for first few sentences\n",
        "for i, sentence in enumerate(sentences[:5]):\n",
        "    score = sentence_scores.get(sentence, 0)\n",
        "    print(f\"Sentence {i+1}: Score = {score:.3f}\")\n",
        "    print(f\"  Text: {sentence[:70]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ySWl6uBn4X2Z",
        "outputId": "f476521f-a3ec-44f6-ae30-b55e9551c1d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Scores:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Select top sentences to form summary\n",
        "\n",
        "# Calculate how many sentences to include in summary (e.g., 30% of original)\n",
        "summary_percentage = 0.3\n",
        "num_summary_sentences = max(1, int(len(sentences) * summary_percentage))\n",
        "\n",
        "print(f\"\\nOriginal sentences: {len(sentences)}\")\n",
        "print(f\"Summary sentences: {num_summary_sentences}\")\n",
        "\n",
        "# Select top scoring sentences\n",
        "summary_sentences = heapq.nlargest(num_summary_sentences,\n",
        "                                  sentence_scores,\n",
        "                                  key=sentence_scores.get)\n",
        "\n",
        "print(\"\\nSelected sentences (unsorted):\")\n",
        "for i, sentence in enumerate(summary_sentences):\n",
        "    print(f\"{i+1}. {sentence[:70]}...\")\n",
        "\n",
        "# Sort sentences in original order\n",
        "summary_sentences.sort(key=lambda x: sentences.index(x))\n",
        "\n",
        "print(\"\\nSelected sentences (in original order):\")\n",
        "for i, sentence in enumerate(summary_sentences):\n",
        "    print(f\"{i+1}. {sentence[:70]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rC59EmlD4YQP",
        "outputId": "a9ef6b21-3528-4f48-a9af-adede60ba148"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Original sentences: 0\n",
            "Summary sentences: 1\n",
            "\n",
            "Selected sentences (unsorted):\n",
            "\n",
            "Selected sentences (in original order):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Display summarized text\n",
        "\n",
        "# Join sentences to form summary\n",
        "summary = ' '.join(summary_sentences)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(summary)\n",
        "\n",
        "# Display statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATISTICS\")\n",
        "print(\"=\"*60)\n",
        "original_words = len(words)\n",
        "summary_words = len(word_tokenize(summary))\n",
        "original_sentences = len(sentences)\n",
        "summary_sentences_count = len(summary_sentences)\n",
        "\n",
        "print(f\"Original text: {original_words} words, {original_sentences} sentences\")\n",
        "print(f\"Summary text: {summary_words} words, {summary_sentences_count} sentences\")\n",
        "\n",
        "# Add checks to prevent ZeroDivisionError if original_words or original_sentences is 0\n",
        "if original_words > 0:\n",
        "    print(f\"Compression ratio: {summary_words/original_words*100:.1f}%\")\n",
        "else:\n",
        "    print(\"Compression ratio: N/A (Original text has no words)\")\n",
        "\n",
        "if original_sentences > 0:\n",
        "    print(f\"Summary keeps {summary_sentences_count/original_sentences*100:.1f}% of sentences\")\n",
        "else:\n",
        "    print(\"Summary keeps: N/A (Original text has no sentences)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2tJGJ5c4YrA",
        "outputId": "3f1531d8-79ac-4ff2-d994-731cc98a6d06"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FINAL SUMMARY\n",
            "============================================================\n",
            "\n",
            "\n",
            "============================================================\n",
            "STATISTICS\n",
            "============================================================\n",
            "Original text: 0 words, 0 sentences\n",
            "Summary text: 0 words, 0 sentences\n",
            "Compression ratio: N/A (Original text has no words)\n",
            "Summary keeps: N/A (Original text has no sentences)\n"
          ]
        }
      ]
    }
  ]
}