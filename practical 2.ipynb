{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8jfwJKOZIL6",
        "outputId": "d778a841-ce71-49e8-d70c-28b6e0b8491b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "STEP 1: Installing NLTK\n",
            "============================================================\n",
            "✓ NLTK installed successfully!\n",
            "\n",
            "============================================================\n",
            "STEP 2: Downloading NLTK datasets\n",
            "============================================================\n",
            "✓ Downloaded 'punkt' - Tokenizer models\n",
            "✓ Downloaded 'stopwords' - Stopwords corpus\n",
            "✓ Downloaded 'wordnet' - WordNet lexical database\n",
            "\n",
            "Setup complete! Now run the other scripts.\n"
          ]
        }
      ],
      "source": [
        "# setup.py - Step 1 & 2: Install and download NLTK resources\n",
        "import nltk\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_nltk():\n",
        "    \"\"\"Step 1: Install NLTK using pip\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 1: Installing NLTK\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"nltk\"])\n",
        "        print(\"✓ NLTK installed successfully!\")\n",
        "    except:\n",
        "        print(\"⚠ Could not install NLTK via pip\")\n",
        "        print(\"Please run manually: pip install nltk\")\n",
        "    print()\n",
        "\n",
        "def download_datasets():\n",
        "    \"\"\"Step 2: Download required NLTK datasets\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 2: Downloading NLTK datasets\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    datasets = {\n",
        "        'punkt': 'Tokenizer models',\n",
        "        'stopwords': 'Stopwords corpus',\n",
        "        'wordnet': 'WordNet lexical database'\n",
        "    }\n",
        "\n",
        "    for dataset, description in datasets.items():\n",
        "        try:\n",
        "            nltk.download(dataset, quiet=True)\n",
        "            print(f\"✓ Downloaded '{dataset}' - {description}\")\n",
        "        except:\n",
        "            print(f\"⚠ Error downloading '{dataset}'\")\n",
        "    print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    install_nltk()\n",
        "    download_datasets()\n",
        "    print(\"Setup complete! Now run the other scripts.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing.py - Steps 3-8: Complete preprocessing pipeline\n",
        "import nltk\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Download punkt_tab resource specifically\n",
        "try:\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    print(\"✓ Downloaded 'punkt_tab' resource.\")\n",
        "except:\n",
        "    print(\"⚠ Error downloading 'punkt_tab'.\")\n",
        "\n",
        "def load_sample_corpus():\n",
        "    \"\"\"Step 3: Load a sample text corpus\"\"\"\n",
        "    sample_text = \"\"\"Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
        "    It helps computers understand, interpret, and manipulate human language.\n",
        "    NLP is used in many applications like chatbots, sentiment analysis, and machine translation.\n",
        "    Researchers are constantly working on improving NLP techniques to make them more accurate.\"\"\"\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 3: SAMPLE TEXT CORPUS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(sample_text)\n",
        "    print()\n",
        "    return sample_text\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Step 4: Perform tokenization\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 4: TOKENIZATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    tokens = word_tokenize(text)\n",
        "    print(f\"Total tokens: {len(tokens)}\")\n",
        "    print(\"Tokens:\", tokens)\n",
        "    print()\n",
        "    return tokens\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    \"\"\"Step 5: Remove stopwords\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 5: STOPWORD REMOVAL\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    print(f\"Total stopwords in NLTK: {len(stop_words)}\")\n",
        "    print(f\"Sample stopwords: {list(stop_words)[:10]}...\")\n",
        "\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "    # Also remove punctuation\n",
        "    filtered_tokens = [word for word in filtered_tokens if word not in string.punctuation]\n",
        "\n",
        "    print(f\"\\nTokens after stopword removal: {len(filtered_tokens)}\")\n",
        "    print(\"Filtered tokens:\", filtered_tokens)\n",
        "    print()\n",
        "    return filtered_tokens\n",
        "\n",
        "def apply_stemming(tokens):\n",
        "    \"\"\"Step 6: Apply stemming\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 6: STEMMING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    print(\"Original → Stemmed\")\n",
        "    print(\"-\" * 30)\n",
        "    for original, stemmed in zip(tokens[:15], stemmed_tokens[:15]):\n",
        "        if original != stemmed:\n",
        "            print(f\"{original:<15} → {stemmed}\")\n",
        "\n",
        "    print(f\"\\nAll stemmed tokens: {stemmed_tokens}\")\n",
        "    print()\n",
        "    return stemmed_tokens\n",
        "\n",
        "def apply_lemmatization(tokens):\n",
        "    \"\"\"Step 7: Apply lemmatization\"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 7: LEMMATIZATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    print(\"Original → Lemmatized\")\n",
        "    print(\"-\" * 30)\n",
        "    for original, lemma in zip(tokens[:15], lemmatized_tokens[:15]):\n",
        "        if original != lemma:\n",
        "            print(f\"{original:<15} → {lemma}\")\n",
        "\n",
        "    print(f\"\\nAll lemmatized tokens: {lemmatized_tokens}\")\n",
        "    print()\n",
        "    return lemmatized_tokens\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function to run the complete preprocessing pipeline\"\"\"\n",
        "    print(\"TEXT PREPROCESSING PRACTICAL\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Run the complete pipeline\n",
        "    text = load_sample_corpus()\n",
        "    tokens = tokenize_text(text)\n",
        "    filtered_tokens = remove_stopwords(tokens)\n",
        "    stemmed_tokens = apply_stemming(filtered_tokens)\n",
        "    lemmatized_tokens = apply_lemmatization(filtered_tokens)\n",
        "\n",
        "    # Step 8: Display final comparison\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 8: FINAL RESULTS COMPARISON\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    print(\"\\nCOMPARISON TABLE:\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'Stage':<20} | {'Token Count':<15} | {'Sample'}\")\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"{'Original Text':<20} | {len(text.split()):<15} | {text[:30]}...\")\n",
        "    print(f\"{'After Tokenization':<20} | {len(tokens):<15} | {tokens[:5]}...\")\n",
        "    print(f\"{'After Stopword Removal':<20} | {len(filtered_tokens):<15} | {filtered_tokens[:5]}...\")\n",
        "    print(f\"{'After Stemming':<20} | {len(stemmed_tokens):<15} | {stemmed_tokens[:5]}...\")\n",
        "    print(f\"{'After Lemmatization':<20} | {len(lemmatized_tokens):<15} | {lemmatized_tokens[:5]}...\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"PROCESSING COMPLETE!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwFyaSeYZe-V",
        "outputId": "2474de00-ecee-426b-8169-d1ff4327f3c0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Downloaded 'punkt_tab' resource.\n",
            "TEXT PREPROCESSING PRACTICAL\n",
            "============================================================\n",
            "============================================================\n",
            "STEP 3: SAMPLE TEXT CORPUS\n",
            "============================================================\n",
            "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
            "    It helps computers understand, interpret, and manipulate human language.\n",
            "    NLP is used in many applications like chatbots, sentiment analysis, and machine translation.\n",
            "    Researchers are constantly working on improving NLP techniques to make them more accurate.\n",
            "\n",
            "============================================================\n",
            "STEP 4: TOKENIZATION\n",
            "============================================================\n",
            "Total tokens: 56\n",
            "Tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', '.', 'It', 'helps', 'computers', 'understand', ',', 'interpret', ',', 'and', 'manipulate', 'human', 'language', '.', 'NLP', 'is', 'used', 'in', 'many', 'applications', 'like', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'Researchers', 'are', 'constantly', 'working', 'on', 'improving', 'NLP', 'techniques', 'to', 'make', 'them', 'more', 'accurate', '.']\n",
            "\n",
            "============================================================\n",
            "STEP 5: STOPWORD REMOVAL\n",
            "============================================================\n",
            "Total stopwords in NLTK: 198\n",
            "Sample stopwords: [\"mustn't\", \"shan't\", \"they've\", 'again', 'will', 'she', 'up', 'no', 'through', \"we'd\"]...\n",
            "\n",
            "Tokens after stopword removal: 33\n",
            "Filtered tokens: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'Artificial', 'Intelligence', 'helps', 'computers', 'understand', 'interpret', 'manipulate', 'human', 'language', 'NLP', 'used', 'many', 'applications', 'like', 'chatbots', 'sentiment', 'analysis', 'machine', 'translation', 'Researchers', 'constantly', 'working', 'improving', 'NLP', 'techniques', 'make', 'accurate']\n",
            "\n",
            "============================================================\n",
            "STEP 6: STEMMING\n",
            "============================================================\n",
            "Original → Stemmed\n",
            "------------------------------\n",
            "Natural         → natur\n",
            "Language        → languag\n",
            "Processing      → process\n",
            "NLP             → nlp\n",
            "fascinating     → fascin\n",
            "Artificial      → artifici\n",
            "Intelligence    → intellig\n",
            "helps           → help\n",
            "computers       → comput\n",
            "manipulate      → manipul\n",
            "language        → languag\n",
            "\n",
            "All stemmed tokens: ['natur', 'languag', 'process', 'nlp', 'fascin', 'field', 'artifici', 'intellig', 'help', 'comput', 'understand', 'interpret', 'manipul', 'human', 'languag', 'nlp', 'use', 'mani', 'applic', 'like', 'chatbot', 'sentiment', 'analysi', 'machin', 'translat', 'research', 'constantli', 'work', 'improv', 'nlp', 'techniqu', 'make', 'accur']\n",
            "\n",
            "============================================================\n",
            "STEP 7: LEMMATIZATION\n",
            "============================================================\n",
            "Original → Lemmatized\n",
            "------------------------------\n",
            "helps           → help\n",
            "computers       → computer\n",
            "\n",
            "All lemmatized tokens: ['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'Artificial', 'Intelligence', 'help', 'computer', 'understand', 'interpret', 'manipulate', 'human', 'language', 'NLP', 'used', 'many', 'application', 'like', 'chatbots', 'sentiment', 'analysis', 'machine', 'translation', 'Researchers', 'constantly', 'working', 'improving', 'NLP', 'technique', 'make', 'accurate']\n",
            "\n",
            "============================================================\n",
            "STEP 8: FINAL RESULTS COMPARISON\n",
            "============================================================\n",
            "\n",
            "COMPARISON TABLE:\n",
            "--------------------------------------------------\n",
            "Stage                | Token Count     | Sample\n",
            "--------------------------------------------------\n",
            "Original Text        | 46              | Natural Language Processing (N...\n",
            "After Tokenization   | 56              | ['Natural', 'Language', 'Processing', '(', 'NLP']...\n",
            "After Stopword Removal | 33              | ['Natural', 'Language', 'Processing', 'NLP', 'fascinating']...\n",
            "After Stemming       | 33              | ['natur', 'languag', 'process', 'nlp', 'fascin']...\n",
            "After Lemmatization  | 33              | ['Natural', 'Language', 'Processing', 'NLP', 'fascinating']...\n",
            "\n",
            "============================================================\n",
            "PROCESSING COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# advanced_preprocessing.py - Additional preprocessing techniques\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def show_advanced_techniques():\n",
        "    \"\"\"Show additional text preprocessing techniques\"\"\"\n",
        "    print(\"ADVANCED TEXT PREPROCESSING TECHNIQUES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    text = \"Natural Language Processing is amazing! It's changing how we interact with computers. The researchers' work on NLP models is revolutionary.\"\n",
        "\n",
        "    # 1. Sentence Tokenization\n",
        "    print(\"\\n1. SENTENCE TOKENIZATION:\")\n",
        "    sentences = sent_tokenize(text)\n",
        "    for i, sentence in enumerate(sentences, 1):\n",
        "        print(f\"Sentence {i}: {sentence}\")\n",
        "\n",
        "    # 2. Different Stemmers\n",
        "    print(\"\\n2. DIFFERENT STEMMERS COMPARISON:\")\n",
        "    words = [\"running\", \"happily\", \"better\", \"went\", \"studies\", \"interesting\"]\n",
        "\n",
        "    porter = PorterStemmer()\n",
        "    snowball = SnowballStemmer(\"english\")\n",
        "\n",
        "    print(f\"{'Word':<15} {'Porter':<15} {'Snowball':<15}\")\n",
        "    print(\"-\" * 45)\n",
        "    for word in words:\n",
        "        print(f\"{word:<15} {porter.stem(word):<15} {snowball.stem(word):<15}\")\n",
        "\n",
        "    # 3. Lemmatization with POS tags\n",
        "    print(\"\\n3. LEMMATIZATION WITH POS TAGS:\")\n",
        "    from nltk import pos_tag\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Sample words with different POS\n",
        "    test_words = [(\"running\", \"v\"), (\"better\", \"a\"), (\"dogs\", \"n\"), (\"running\", \"n\")]\n",
        "\n",
        "    print(f\"{'Word':<10} {'POS':<5} {'Lemma':<15}\")\n",
        "    print(\"-\" * 30)\n",
        "    for word, pos in test_words:\n",
        "        # Convert POS to WordNet format\n",
        "        if pos.startswith('v'):\n",
        "            pos_wordnet = wordnet.VERB\n",
        "        elif pos.startswith('a'):\n",
        "            pos_wordnet = wordnet.ADJ\n",
        "        elif pos.startswith('n'):\n",
        "            pos_wordnet = wordnet.NOUN\n",
        "        else:\n",
        "            pos_wordnet = wordnet.NOUN\n",
        "\n",
        "        lemma = lemmatizer.lemmatize(word, pos=pos_wordnet)\n",
        "        print(f\"{word:<10} {pos:<5} {lemma:<15}\")\n",
        "\n",
        "    # 4. Custom stopwords\n",
        "    print(\"\\n4. CUSTOM STOPWORDS EXAMPLE:\")\n",
        "    custom_stopwords = set(stopwords.words('english'))\n",
        "    custom_stopwords.update(['natural', 'language', 'processing'])\n",
        "\n",
        "    sample = \"Natural Language Processing helps in understanding natural language patterns.\"\n",
        "    tokens = word_tokenize(sample.lower())\n",
        "    filtered = [word for word in tokens if word not in custom_stopwords and word not in string.punctuation]\n",
        "\n",
        "    print(f\"Original: {sample}\")\n",
        "    print(f\"Filtered: {filtered}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    show_advanced_techniques()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TyFOeQdHZnIX",
        "outputId": "863b0b04-3bf7-4ca8-a26a-eb3bdda9c90d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADVANCED TEXT PREPROCESSING TECHNIQUES\n",
            "============================================================\n",
            "\n",
            "1. SENTENCE TOKENIZATION:\n",
            "Sentence 1: Natural Language Processing is amazing!\n",
            "Sentence 2: It's changing how we interact with computers.\n",
            "Sentence 3: The researchers' work on NLP models is revolutionary.\n",
            "\n",
            "2. DIFFERENT STEMMERS COMPARISON:\n",
            "Word            Porter          Snowball       \n",
            "---------------------------------------------\n",
            "running         run             run            \n",
            "happily         happili         happili        \n",
            "better          better          better         \n",
            "went            went            went           \n",
            "studies         studi           studi          \n",
            "interesting     interest        interest       \n",
            "\n",
            "3. LEMMATIZATION WITH POS TAGS:\n",
            "Word       POS   Lemma          \n",
            "------------------------------\n",
            "running    v     run            \n",
            "better     a     good           \n",
            "dogs       n     dog            \n",
            "running    n     running        \n",
            "\n",
            "4. CUSTOM STOPWORDS EXAMPLE:\n",
            "Original: Natural Language Processing helps in understanding natural language patterns.\n",
            "Filtered: ['helps', 'understanding', 'patterns']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run_all.py - Run all preprocessing steps\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def run_script(script_name, description):\n",
        "    \"\"\"Run a Python script\"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"RUNNING: {description}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        subprocess.run([sys.executable, script_name], check=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Error running {script_name}: {e}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Script {script_name} not found!\")\n",
        "\n",
        "def main():\n",
        "    print(\"TEXT PREPROCESSING PRACTICAL - COMPLETE EXECUTION\")\n",
        "\n",
        "    # Run setup first\n",
        "    run_script(\"setup.py\", \"Setup and Installation\")\n",
        "\n",
        "    # Run main preprocessing\n",
        "    run_script(\"preprocessing.py\", \"Main Preprocessing Pipeline\")\n",
        "\n",
        "    # Run advanced techniques\n",
        "    response = input(\"\\nDo you want to see advanced techniques? (y/n): \")\n",
        "    if response.lower() == 'y':\n",
        "        run_script(\"advanced_preprocessing.py\", \"Advanced Preprocessing Techniques\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"PRACTICAL COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1PvlF1qZ4ZQ",
        "outputId": "c6ca9e00-c0a0-4397-9958-5c7e1e9fca7d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TEXT PREPROCESSING PRACTICAL - COMPLETE EXECUTION\n",
            "\n",
            "============================================================\n",
            "RUNNING: Setup and Installation\n",
            "============================================================\n",
            "Error running setup.py: Command '['/usr/bin/python3', 'setup.py']' returned non-zero exit status 2.\n",
            "\n",
            "============================================================\n",
            "RUNNING: Main Preprocessing Pipeline\n",
            "============================================================\n",
            "Error running preprocessing.py: Command '['/usr/bin/python3', 'preprocessing.py']' returned non-zero exit status 2.\n",
            "\n",
            "Do you want to see advanced techniques? (y/n): nnn\n",
            "\n",
            "============================================================\n",
            "PRACTICAL COMPLETED SUCCESSFULLY!\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}