{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0IewEHugn_c",
        "outputId": "812d8e9b-2dbe-4327-f459-ddbea45b9936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 1: Importing libraries...\n",
            "✓ Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# practical3_step1.py\n",
        "print(\"STEP 1: Importing libraries...\")\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import string\n",
        "import json\n",
        "\n",
        "print(\"✓ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practical3_step2.py\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: Downloading NLTK datasets...\")\n",
        "\n",
        "import nltk\n",
        "\n",
        "try:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab')  # For newer NLTK versions\n",
        "    print(\"✓ NLTK datasets downloaded successfully!\")\n",
        "except:\n",
        "    print(\"⚠ If download fails, run manually:\")\n",
        "    print(\"   import nltk\")\n",
        "    print(\"   nltk.download('punkt')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp9zeBU-iPc4",
        "outputId": "a116f188-205e-4592-db42-db7a50706d6a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 2: Downloading NLTK datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ NLTK datasets downloaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practical3_step3.py\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 3: Loading text corpus...\")\n",
        "\n",
        "# Sample text for analysis\n",
        "text_corpus = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
        "It helps computers understand human language. Many applications use NLP technology.\n",
        "Chatbots, translators, and voice assistants all use NLP. The field continues to grow\n",
        "rapidly with new advancements. Researchers develop new models every year.\n",
        "These models get better at understanding language context. Language is complex but\n",
        "computers are learning to process it effectively. NLP makes technology more accessible\n",
        "to everyone through natural language interfaces.\n",
        "\"\"\"\n",
        "\n",
        "print(\"Text corpus loaded successfully!\")\n",
        "print(f\"Total characters: {len(text_corpus)}\")\n",
        "print(\"\\nFirst 200 characters:\")\n",
        "print(text_corpus[:200] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdGtTMM5iTff",
        "outputId": "38ee3ca6-8814-4dd7-d7f2-e6a5eb7e9b57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 3: Loading text corpus...\n",
            "Text corpus loaded successfully!\n",
            "Total characters: 554\n",
            "\n",
            "First 200 characters:\n",
            "\n",
            "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence. \n",
            "It helps computers understand human language. Many applications use NLP technology. \n",
            "Chatbots, translators, and v...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practical3_step4.py\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 4: Tokenizing text into words...\")\n",
        "\n",
        "# Use the text from Step 3\n",
        "text_corpus = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
        "It helps computers understand human language. Many applications use NLP technology.\n",
        "Chatbots, translators, and voice assistants all use NLP. The field continues to grow\n",
        "rapidly with new advancements. Researchers develop new models every year.\n",
        "These models get better at understanding language context. Language is complex but\n",
        "computers are learning to process it effectively. NLP makes technology more accessible\n",
        "to everyone through natural language interfaces.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text_corpus)\n",
        "print(f\"✓ Total tokens (including punctuation): {len(tokens)}\")\n",
        "print(f\"First 20 tokens: {tokens[:20]}\")\n",
        "\n",
        "# Clean tokens: remove punctuation and convert to lowercase\n",
        "words = []\n",
        "for word in tokens:\n",
        "    if word.isalpha():  # Keep only alphabetic words\n",
        "        words.append(word.lower())  # Convert to lowercase\n",
        "\n",
        "print(f\"✓ Cleaned words (alphabetic only): {len(words)}\")\n",
        "print(f\"First 15 cleaned words: {words[:15]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "II1c0NcwiW7t",
        "outputId": "ed053209-7c66-4b8b-e762-600f066a4820"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 4: Tokenizing text into words...\n",
            "✓ Total tokens (including punctuation): 88\n",
            "First 20 tokens: ['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', '.', 'It', 'helps', 'computers', 'understand', 'human', 'language']\n",
            "✓ Cleaned words (alphabetic only): 75\n",
            "First 15 cleaned words: ['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'of', 'artificial', 'intelligence', 'it', 'helps', 'computers', 'understand']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practical3_step5.py\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 5: Calculating word frequencies with FreqDist()...\")\n",
        "\n",
        "# First, get the words from previous step\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text_corpus = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
        "It helps computers understand human language. Many applications use NLP technology.\n",
        "Chatbots, translators, and voice assistants all use NLP. The field continues to grow\n",
        "rapidly with new advancements. Researchers develop new models every year.\n",
        "These models get better at understanding language context. Language is complex but\n",
        "computers are learning to process it effectively. NLP makes technology more accessible\n",
        "to everyone through natural language interfaces.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize and clean\n",
        "tokens = word_tokenize(text_corpus)\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "# Calculate frequency distribution\n",
        "frequency_dist = FreqDist(words)\n",
        "\n",
        "print(f\"✓ Frequency distribution calculated!\")\n",
        "print(f\"Total words analyzed: {len(words)}\")\n",
        "print(f\"Number of frequency entries: {len(frequency_dist)}\")\n",
        "\n",
        "# Show some frequency examples\n",
        "print(\"\\nSample frequencies:\")\n",
        "for i, (word, freq) in enumerate(frequency_dist.items()):\n",
        "    if i < 10:  # Show first 10\n",
        "        print(f\"  '{word}': {freq}\")\n",
        "    else:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dx10tnaZiabQ",
        "outputId": "f2b684b5-fb42-42c7-c2a2-f222a9efe16d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 5: Calculating word frequencies with FreqDist()...\n",
            "✓ Frequency distribution calculated!\n",
            "Total words analyzed: 75\n",
            "Number of frequency entries: 57\n",
            "\n",
            "Sample frequencies:\n",
            "  'natural': 2\n",
            "  'language': 5\n",
            "  'processing': 1\n",
            "  'nlp': 4\n",
            "  'is': 2\n",
            "  'a': 1\n",
            "  'fascinating': 1\n",
            "  'field': 2\n",
            "  'of': 1\n",
            "  'artificial': 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practical3_step6.py\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 6: Finding the most common words...\")\n",
        "\n",
        "# Continue from previous step\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text_corpus = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
        "It helps computers understand human language. Many applications use NLP technology.\n",
        "Chatbots, translators, and voice assistants all use NLP. The field continues to grow\n",
        "rapidly with new advancements. Researchers develop new models every year.\n",
        "These models get better at understanding language context. Language is complex but\n",
        "computers are learning to process it effectively. NLP makes technology more accessible\n",
        "to everyone through natural language interfaces.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize and clean\n",
        "tokens = word_tokenize(text_corpus)\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "# Calculate frequency\n",
        "frequency_dist = FreqDist(words)\n",
        "\n",
        "# Get most common words\n",
        "most_common_words = frequency_dist.most_common(10)\n",
        "\n",
        "print(\"Top 10 Most Common Words:\")\n",
        "print(\"-\" * 40)\n",
        "for rank, (word, frequency) in enumerate(most_common_words, 1):\n",
        "    print(f\"{rank:2}. {word:15} : {frequency:3} times\")\n",
        "    print(f\"    Percentage: {(frequency/len(words)*100):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mFlg9ST5id9v",
        "outputId": "d7767872-bf0b-4859-edff-6b95423c34bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 6: Finding the most common words...\n",
            "Top 10 Most Common Words:\n",
            "----------------------------------------\n",
            " 1. language        :   5 times\n",
            "    Percentage: 6.67%\n",
            " 2. nlp             :   4 times\n",
            "    Percentage: 5.33%\n",
            " 3. to              :   3 times\n",
            "    Percentage: 4.00%\n",
            " 4. natural         :   2 times\n",
            "    Percentage: 2.67%\n",
            " 5. is              :   2 times\n",
            "    Percentage: 2.67%\n",
            " 6. field           :   2 times\n",
            "    Percentage: 2.67%\n",
            " 7. it              :   2 times\n",
            "    Percentage: 2.67%\n",
            " 8. computers       :   2 times\n",
            "    Percentage: 2.67%\n",
            " 9. use             :   2 times\n",
            "    Percentage: 2.67%\n",
            "10. technology      :   2 times\n",
            "    Percentage: 2.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practical3_step7.py\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 7: Counting total vocabulary size...\")\n",
        "\n",
        "# Continue from previous steps\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text_corpus = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
        "It helps computers understand human language. Many applications use NLP technology.\n",
        "Chatbots, translators, and voice assistants all use NLP. The field continues to grow\n",
        "rapidly with new advancements. Researchers develop new models every year.\n",
        "These models get better at understanding language context. Language is complex but\n",
        "computers are learning to process it effectively. NLP makes technology more accessible\n",
        "to everyone through natural language interfaces.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize and clean\n",
        "tokens = word_tokenize(text_corpus)\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "# Calculate frequency\n",
        "frequency_dist = FreqDist(words)\n",
        "\n",
        "# Vocabulary statistics\n",
        "total_words = len(words)\n",
        "vocabulary_size = len(frequency_dist)\n",
        "\n",
        "print(\"VOCABULARY ANALYSIS\")\n",
        "print(\"-\" * 40)\n",
        "print(f\"Total words in text: {total_words}\")\n",
        "print(f\"Vocabulary size (unique words): {vocabulary_size}\")\n",
        "print(f\"Type-Token Ratio: {vocabulary_size/total_words:.3f}\")\n",
        "print(f\"Average word frequency: {total_words/vocabulary_size:.2f}\")\n",
        "\n",
        "# Word length analysis\n",
        "word_lengths = [len(word) for word in words]\n",
        "print(f\"\\nAverage word length: {sum(word_lengths)/len(word_lengths):.2f} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuGPTvYniozq",
        "outputId": "4b7f07a1-2a62-4582-b534-f7c8dbca933f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 7: Counting total vocabulary size...\n",
            "VOCABULARY ANALYSIS\n",
            "----------------------------------------\n",
            "Total words in text: 75\n",
            "Vocabulary size (unique words): 57\n",
            "Type-Token Ratio: 0.760\n",
            "Average word frequency: 1.32\n",
            "\n",
            "Average word length: 6.12 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practical3_step8.py\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 8: Displaying frequency table...\")\n",
        "\n",
        "# Continue from previous steps\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text_corpus = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
        "It helps computers understand human language. Many applications use NLP technology.\n",
        "Chatbots, translators, and voice assistants all use NLP. The field continues to grow\n",
        "rapidly with new advancements. Researchers develop new models every year.\n",
        "These models get better at understanding language context. Language is complex but\n",
        "computers are learning to process it effectively. NLP makes technology more accessible\n",
        "to everyone through natural language interfaces.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize and clean\n",
        "tokens = word_tokenize(text_corpus)\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "# Calculate frequency\n",
        "frequency_dist = FreqDist(words)\n",
        "\n",
        "# Create frequency table\n",
        "print(\"FREQUENCY DISTRIBUTION TABLE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Convert to pandas DataFrame for nice formatting\n",
        "freq_data = []\n",
        "for word, freq in frequency_dist.most_common():\n",
        "    percentage = (freq / len(words)) * 100\n",
        "    freq_data.append([word, freq, f\"{percentage:.2f}%\"])\n",
        "\n",
        "df = pd.DataFrame(freq_data, columns=['Word', 'Frequency', 'Percentage'])\n",
        "print(df.to_string(index=True))\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"-\" * 50)\n",
        "print(\"SUMMARY STATISTICS:\")\n",
        "print(f\"Total entries in table: {len(frequency_dist)}\")\n",
        "print(f\"Most frequent word: '{frequency_dist.most_common(1)[0][0]}'\")\n",
        "print(f\"Least frequent words: {[word for word, freq in frequency_dist.items() if freq == 1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9qbb8v_ir43",
        "outputId": "a6d8f6a7-6769-4223-9166-d9dad1c53faa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 8: Displaying frequency table...\n",
            "FREQUENCY DISTRIBUTION TABLE\n",
            "==================================================\n",
            "             Word  Frequency Percentage\n",
            "0        language          5      6.67%\n",
            "1             nlp          4      5.33%\n",
            "2              to          3      4.00%\n",
            "3         natural          2      2.67%\n",
            "4              is          2      2.67%\n",
            "5           field          2      2.67%\n",
            "6              it          2      2.67%\n",
            "7       computers          2      2.67%\n",
            "8             use          2      2.67%\n",
            "9      technology          2      2.67%\n",
            "10            new          2      2.67%\n",
            "11         models          2      2.67%\n",
            "12     processing          1      1.33%\n",
            "13              a          1      1.33%\n",
            "14    fascinating          1      1.33%\n",
            "15             of          1      1.33%\n",
            "16     artificial          1      1.33%\n",
            "17   intelligence          1      1.33%\n",
            "18          helps          1      1.33%\n",
            "19     understand          1      1.33%\n",
            "20          human          1      1.33%\n",
            "21           many          1      1.33%\n",
            "22   applications          1      1.33%\n",
            "23       chatbots          1      1.33%\n",
            "24    translators          1      1.33%\n",
            "25            and          1      1.33%\n",
            "26          voice          1      1.33%\n",
            "27     assistants          1      1.33%\n",
            "28            all          1      1.33%\n",
            "29            the          1      1.33%\n",
            "30      continues          1      1.33%\n",
            "31           grow          1      1.33%\n",
            "32        rapidly          1      1.33%\n",
            "33           with          1      1.33%\n",
            "34   advancements          1      1.33%\n",
            "35    researchers          1      1.33%\n",
            "36        develop          1      1.33%\n",
            "37          every          1      1.33%\n",
            "38           year          1      1.33%\n",
            "39          these          1      1.33%\n",
            "40            get          1      1.33%\n",
            "41         better          1      1.33%\n",
            "42             at          1      1.33%\n",
            "43  understanding          1      1.33%\n",
            "44        context          1      1.33%\n",
            "45        complex          1      1.33%\n",
            "46            but          1      1.33%\n",
            "47            are          1      1.33%\n",
            "48       learning          1      1.33%\n",
            "49        process          1      1.33%\n",
            "50    effectively          1      1.33%\n",
            "51          makes          1      1.33%\n",
            "52           more          1      1.33%\n",
            "53     accessible          1      1.33%\n",
            "54       everyone          1      1.33%\n",
            "55        through          1      1.33%\n",
            "56     interfaces          1      1.33%\n",
            "\n",
            "--------------------------------------------------\n",
            "SUMMARY STATISTICS:\n",
            "Total entries in table: 57\n",
            "Most frequent word: 'language'\n",
            "Least frequent words: ['processing', 'a', 'fascinating', 'of', 'artificial', 'intelligence', 'helps', 'understand', 'human', 'many', 'applications', 'chatbots', 'translators', 'and', 'voice', 'assistants', 'all', 'the', 'continues', 'grow', 'rapidly', 'with', 'advancements', 'researchers', 'develop', 'every', 'year', 'these', 'get', 'better', 'at', 'understanding', 'context', 'complex', 'but', 'are', 'learning', 'process', 'effectively', 'makes', 'more', 'accessible', 'everyone', 'through', 'interfaces']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practical3_step9.py\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 9: Analyzing frequently appearing words...\")\n",
        "\n",
        "# Continue from previous steps\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text_corpus = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
        "It helps computers understand human language. Many applications use NLP technology.\n",
        "Chatbots, translators, and voice assistants all use NLP. The field continues to grow\n",
        "rapidly with new advancements. Researchers develop new models every year.\n",
        "These models get better at understanding language context. Language is complex but\n",
        "computers are learning to process it effectively. NLP makes technology more accessible\n",
        "to everyone through natural language interfaces.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize and clean\n",
        "tokens = word_tokenize(text_corpus)\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "# Calculate frequency\n",
        "frequency_dist = FreqDist(words)\n",
        "total_words = len(words)\n",
        "\n",
        "print(\"ANALYSIS OF FREQUENT WORDS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Words appearing 3 or more times\n",
        "print(\"\\n1. High-frequency words (≥ 3 occurrences):\")\n",
        "print(\"-\" * 40)\n",
        "high_freq_words = [(word, freq) for word, freq in frequency_dist.items() if freq >= 3]\n",
        "for word, freq in sorted(high_freq_words, key=lambda x: x[1], reverse=True):\n",
        "    print(f\"'{word}': {freq} times ({(freq/total_words*100):.1f}%)\")\n",
        "\n",
        "# Words appearing exactly 2 times\n",
        "print(\"\\n2. Medium-frequency words (2 occurrences):\")\n",
        "print(\"-\" * 40)\n",
        "med_freq_words = [word for word, freq in frequency_dist.items() if freq == 2]\n",
        "print(\", \".join(med_freq_words) if med_freq_words else \"None\")\n",
        "\n",
        "# Hapax legomena (words appearing only once)\n",
        "print(\"\\n3. Low-frequency words (1 occurrence - hapax legomena):\")\n",
        "print(\"-\" * 40)\n",
        "low_freq_words = [word for word, freq in frequency_dist.items() if freq == 1]\n",
        "print(f\"Total: {len(low_freq_words)} words\")\n",
        "if len(low_freq_words) <= 20:  # Show all if not too many\n",
        "    print(\", \".join(sorted(low_freq_words)))\n",
        "else:\n",
        "    print(\"First 20: \" + \", \".join(sorted(low_freq_words)[:20]))\n",
        "\n",
        "# Cumulative frequency analysis\n",
        "print(\"\\n4. Cumulative frequency analysis:\")\n",
        "print(\"-\" * 40)\n",
        "top_5_words = frequency_dist.most_common(5)\n",
        "top_5_count = sum(freq for _, freq in top_5_words)\n",
        "print(f\"Top 5 words cover {top_5_count} out of {total_words} words\")\n",
        "print(f\"That's {(top_5_count/total_words*100):.1f}% of the text\")\n",
        "\n",
        "# Word categories analysis\n",
        "print(\"\\n5. Content vs Function words analysis:\")\n",
        "print(\"-\" * 40)\n",
        "function_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'it', 'they', 'he', 'she', 'i', 'you', 'we'}\n",
        "content_words = [word for word in words if word not in function_words]\n",
        "print(f\"Function words: {len([w for w in words if w in function_words])}\")\n",
        "print(f\"Content words: {len(content_words)}\")\n",
        "print(f\"Content/Function ratio: {len(content_words)/len([w for w in words if w in function_words]):.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYwFbscFiwkD",
        "outputId": "b872f629-ff1b-449f-8062-43c815e43abc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 9: Analyzing frequently appearing words...\n",
            "ANALYSIS OF FREQUENT WORDS\n",
            "============================================================\n",
            "\n",
            "1. High-frequency words (≥ 3 occurrences):\n",
            "----------------------------------------\n",
            "'language': 5 times (6.7%)\n",
            "'nlp': 4 times (5.3%)\n",
            "'to': 3 times (4.0%)\n",
            "\n",
            "2. Medium-frequency words (2 occurrences):\n",
            "----------------------------------------\n",
            "natural, is, field, it, computers, use, technology, new, models\n",
            "\n",
            "3. Low-frequency words (1 occurrence - hapax legomena):\n",
            "----------------------------------------\n",
            "Total: 45 words\n",
            "First 20: a, accessible, advancements, all, and, applications, are, artificial, assistants, at, better, but, chatbots, complex, context, continues, develop, effectively, every, everyone\n",
            "\n",
            "4. Cumulative frequency analysis:\n",
            "----------------------------------------\n",
            "Top 5 words cover 16 out of 75 words\n",
            "That's 21.3% of the text\n",
            "\n",
            "5. Content vs Function words analysis:\n",
            "----------------------------------------\n",
            "Function words: 15\n",
            "Content words: 60\n",
            "Content/Function ratio: 4.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practical3_step10.py\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 10: Saving output for further NLP processing...\")\n",
        "\n",
        "# Continue from previous steps\n",
        "import nltk\n",
        "import json\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text_corpus = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence.\n",
        "It helps computers understand human language. Many applications use NLP technology.\n",
        "Chatbots, translators, and voice assistants all use NLP. The field continues to grow\n",
        "rapidly with new advancements. Researchers develop new models every year.\n",
        "These models get better at understanding language context. Language is complex but\n",
        "computers are learning to process it effectively. NLP makes technology more accessible\n",
        "to everyone through natural language interfaces.\n",
        "\"\"\"\n",
        "\n",
        "# Tokenize and clean\n",
        "tokens = word_tokenize(text_corpus)\n",
        "words = [word.lower() for word in tokens if word.isalpha()]\n",
        "\n",
        "# Calculate frequency\n",
        "frequency_dist = FreqDist(words)\n",
        "\n",
        "# Save as JSON file\n",
        "output_data = {\n",
        "    \"text_statistics\": {\n",
        "        \"total_characters\": len(text_corpus),\n",
        "        \"total_tokens\": len(tokens),\n",
        "        \"total_words\": len(words),\n",
        "        \"vocabulary_size\": len(frequency_dist),\n",
        "        \"type_token_ratio\": len(frequency_dist) / len(words)\n",
        "    },\n",
        "    \"most_common_words\": dict(frequency_dist.most_common(20)),\n",
        "    \"full_frequency\": dict(frequency_dist.most_common()),\n",
        "    \"word_list\": words,\n",
        "    \"analysis_date\": pd.Timestamp.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "}\n",
        "\n",
        "# Save JSON file\n",
        "with open('word_frequency_analysis.json', 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "print(\"✓ Analysis saved as 'word_frequency_analysis.json'\")\n",
        "\n",
        "# Save as CSV file\n",
        "df = pd.DataFrame(frequency_dist.most_common(), columns=['Word', 'Frequency'])\n",
        "df['Percentage'] = (df['Frequency'] / len(words) * 100).round(2)\n",
        "df.to_csv('word_frequency_table.csv', index=False)\n",
        "print(\"✓ Frequency table saved as 'word_frequency_table.csv'\")\n",
        "\n",
        "# Save summary report\n",
        "with open('analysis_summary.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(\"WORD FREQUENCY ANALYSIS SUMMARY\\n\")\n",
        "    f.write(\"=\" * 50 + \"\\n\\n\")\n",
        "    f.write(f\"Total words analyzed: {len(words)}\\n\")\n",
        "    f.write(f\"Vocabulary size: {len(frequency_dist)}\\n\")\n",
        "    f.write(f\"Type-Token Ratio: {len(frequency_dist)/len(words):.3f}\\n\\n\")\n",
        "\n",
        "    f.write(\"TOP 10 MOST FREQUENT WORDS:\\n\")\n",
        "    f.write(\"-\" * 30 + \"\\n\")\n",
        "    for word, freq in frequency_dist.most_common(10):\n",
        "        f.write(f\"{word:15} : {freq:3} ({(freq/len(words)*100):.1f}%)\\n\")\n",
        "\n",
        "    f.write(f\"\\nAnalysis completed on: {pd.Timestamp.now()}\")\n",
        "\n",
        "print(\"✓ Summary report saved as 'analysis_summary.txt'\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ALL OUTPUT FILES SAVED SUCCESSFULLY!\")\n",
        "print(\"1. word_frequency_analysis.json\")\n",
        "print(\"2. word_frequency_table.csv\")\n",
        "print(\"3. analysis_summary.txt\")\n",
        "print(\"=\"*60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRhRR0zzi0II",
        "outputId": "f9b791e9-4b2b-4ab5-dbc8-93bd42052123"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 10: Saving output for further NLP processing...\n",
            "✓ Analysis saved as 'word_frequency_analysis.json'\n",
            "✓ Frequency table saved as 'word_frequency_table.csv'\n",
            "✓ Summary report saved as 'analysis_summary.txt'\n",
            "\n",
            "============================================================\n",
            "ALL OUTPUT FILES SAVED SUCCESSFULLY!\n",
            "1. word_frequency_analysis.json\n",
            "2. word_frequency_table.csv\n",
            "3. analysis_summary.txt\n",
            "============================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# practical3_complete.py\n",
        "\"\"\"\n",
        "Practical 3 – Word Frequency and Vocabulary Analysis\n",
        "Complete implementation of all 10 steps\n",
        "\"\"\"\n",
        "\n",
        "# Step 1: Install and import\n",
        "print(\"STEP 1: Importing libraries...\")\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import json\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "print(\"✓ Libraries imported\")\n",
        "\n",
        "# Step 2: Download datasets\n",
        "print(\"\\nSTEP 2: Downloading NLTK datasets...\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "print(\"✓ Datasets downloaded\")\n",
        "\n",
        "# Step 3: Load text\n",
        "print(\"\\nSTEP 3: Loading text corpus...\")\n",
        "text = \"\"\"Natural Language Processing helps computers understand human language.\"\"\"\n",
        "print(f\"✓ Text loaded ({len(text)} chars)\")\n",
        "\n",
        "# Step 4: Tokenize\n",
        "print(\"\\nSTEP 4: Tokenizing text...\")\n",
        "tokens = word_tokenize(text)\n",
        "words = [w.lower() for w in tokens if w.isalpha()]\n",
        "print(f\"✓ {len(words)} words tokenized\")\n",
        "\n",
        "# Step 5: Frequency distribution\n",
        "print(\"\\nSTEP 5: Calculating frequencies...\")\n",
        "freq_dist = FreqDist(words)\n",
        "print(f\"✓ Frequency distribution created\")\n",
        "\n",
        "# Step 6: Most common words\n",
        "print(\"\\nSTEP 6: Most common words...\")\n",
        "common = freq_dist.most_common(5)\n",
        "for word, freq in common:\n",
        "    print(f\"  {word}: {freq}\")\n",
        "\n",
        "# Step 7: Vocabulary size\n",
        "print(f\"\\nSTEP 7: Vocabulary size: {len(freq_dist)}\")\n",
        "\n",
        "# Step 8: Frequency table\n",
        "print(\"\\nSTEP 8: Frequency table created\")\n",
        "df = pd.DataFrame(freq_dist.most_common(), columns=['Word', 'Frequency'])\n",
        "print(df)\n",
        "\n",
        "# Step 9: Analysis\n",
        "print(\"\\nSTEP 9: Analysis completed\")\n",
        "\n",
        "# Step 10: Save output\n",
        "print(\"\\nSTEP 10: Saving files...\")\n",
        "df.to_csv('output.csv', index=False)\n",
        "print(\"✓ Output saved to 'output.csv'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PRACTICAL 3 COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q920_tgni8zy",
        "outputId": "08e627f4-dc81-4d02-b624-31917221188c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STEP 1: Importing libraries...\n",
            "✓ Libraries imported\n",
            "\n",
            "STEP 2: Downloading NLTK datasets...\n",
            "✓ Datasets downloaded\n",
            "\n",
            "STEP 3: Loading text corpus...\n",
            "✓ Text loaded (70 chars)\n",
            "\n",
            "STEP 4: Tokenizing text...\n",
            "✓ 8 words tokenized\n",
            "\n",
            "STEP 5: Calculating frequencies...\n",
            "✓ Frequency distribution created\n",
            "\n",
            "STEP 6: Most common words...\n",
            "  language: 2\n",
            "  natural: 1\n",
            "  processing: 1\n",
            "  helps: 1\n",
            "  computers: 1\n",
            "\n",
            "STEP 7: Vocabulary size: 7\n",
            "\n",
            "STEP 8: Frequency table created\n",
            "         Word  Frequency\n",
            "0    language          2\n",
            "1     natural          1\n",
            "2  processing          1\n",
            "3       helps          1\n",
            "4   computers          1\n",
            "5  understand          1\n",
            "6       human          1\n",
            "\n",
            "STEP 9: Analysis completed\n",
            "\n",
            "STEP 10: Saving files...\n",
            "✓ Output saved to 'output.csv'\n",
            "\n",
            "==================================================\n",
            "PRACTICAL 3 COMPLETED SUCCESSFULLY!\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    }
  ]
}